{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install llama_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pbA-yKdt3lZ",
        "outputId": "2c6252b9-38e8-4918-ebc9-653bf3e7aaed"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_hub in /usr/local/lib/python3.10/dist-packages (0.0.50)\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.10/dist-packages (from llama_hub) (2020.1.16)\n",
            "Requirement already satisfied: llama-index>=0.9.8 in /usr/local/lib/python3.10/dist-packages (from llama_hub) (0.9.8.post1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from llama_hub) (5.9.5)\n",
            "Requirement already satisfied: pyaml<24.0.0,>=23.9.7 in /usr/local/lib/python3.10/dist-packages (from llama_hub) (23.9.7)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from llama_hub) (1.3.4)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (3.8.6)\n",
            "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (4.12.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (0.6.3)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (1.2.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (0.25.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (1.23.5)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (1.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (4.5.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index>=0.9.8->llama_hub) (0.9.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml<24.0.0,>=23.9.7->llama_hub) (6.0.1)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->llama_hub) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index>=0.9.8->llama_hub) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index>=0.9.8->llama_hub) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index>=0.9.8->llama_hub) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index>=0.9.8->llama_hub) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index>=0.9.8->llama_hub) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index>=0.9.8->llama_hub) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index>=0.9.8->llama_hub) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index>=0.9.8->llama_hub) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index>=0.9.8->llama_hub) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index>=0.9.8->llama_hub) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index>=0.9.8->llama_hub) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index>=0.9.8->llama_hub) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index>=0.9.8->llama_hub) (4.66.1)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index>=0.9.8->llama_hub) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index>=0.9.8->llama_hub) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index>=0.9.8->llama_hub) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index>=0.9.8->llama_hub) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index>=0.9.8->llama_hub) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index>=0.9.8->llama_hub) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index>=0.9.8->llama_hub) (3.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index>=0.9.8->llama_hub) (0.14.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index>=0.9.8->llama_hub) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index>=0.9.8->llama_hub) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index>=0.9.8->llama_hub) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index>=0.9.8->llama_hub) (3.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index>=0.9.8->llama_hub) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index>=0.9.8->llama_hub) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>=1.1.0->llama-index>=0.9.8->llama_hub) (1.1.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index>=0.9.8->llama_hub) (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install youtube_transcript_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf0Iy8-fuCPP",
        "outputId": "081736c0-1c07-4628-ca72-022c4981ffc3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8CGkSgBty56",
        "outputId": "58879601-e51d-42af-c41a-36f70df702bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='9023226f-9a97-4cde-b37d-90b85701b9aa', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346', text=\"the imaginative laws scale visual\\nrecognition challenge was a\\nworld-changing competition\\nthat ran from around 2010 to 2017.\\nduring his time the competition acted as\\nthe place to go if you needed to find\\nwhat the current state of the art was in\\nimage classification object localization\\nobject detection as well as that\\n2012 onwards it really acted as the\\nCatalyst of the explosion in deep\\nlearning researchers fine-tuned better\\nperforming computer vision models year\\non year but there was a unquestioned\\nassumption causing problems\\nit was assumed that every new task\\nrequired model fine tuning this required\\na lot of data and a lot of data required\\na lot of capital and time it wasn't\\nuntil recently that this assumption was\\nchallenged and proven wrong the\\nastonishing rise of what are called\\nmulti-modal models has made the what was\\nthought impossible very possible across\\nvarious domains and tasks one of those\\nis called zero shot object detection and\\nlocalization now zero shot refers to\\ntaking a model and applying it to a new\\ndomain without ever fine-tuning it on\\ndata from that new domain so that means\\nwe can take a model who can it maybe it\\nworks in in one domain classification in\\none particular area on one data set and\\nwe can take that same model without any\\nfine tuning and we can use it for object\\ndetection in a completely different\\ndomain without that model seeing any\\ntraining data from that new domain so in\\nthis video we're going to explore how to\\nuse open ai's clip for zero shots object\\ndetection and localization let's begin\\nwith taking a quick look at image\\nclassification now image classification\\ncan kind of be seen as one of the\\nsimplest tasks in visual recognition and\\nit's also the first step on the way to\\nobject detection at his core it's just\\nassigning a categorical label to an\\nimage now moving on from image\\nclassification we have object\\nlocalization object localization is\\nimage classification followed by the\\nidentification of where in the image the\\nspecific object actually is so we are\\nlocalizing the the object now doing that\\nwhere essentially just going to identify\\nthe coordinates on the image I going to\\nreturn the typical approach to this is\\nreturn an image where you have like a\\nbounding box surrounding the the object\\nthat you are looking for and then we\\ntake this one step further to perform\\nobject detection with detection we are\\nlocalizing multiple objects within the\\nimage or we have the capability to\\nidentify multiple objects within the\\nimage so in this example we have cat and\\na dog we would expect with object\\ndetection to identify both the cat and\\nthe dot in the case of us having\\nmultiple dolbs in this image almost Cuts\\nin this image we would also expect the\\nobject detection algorithm to actually\\nidentify each one of those independently\\nnow in the past if we wanted to switch a\\nmodel between anyone these tasks would\\nhave to fine-tune or more data you want\\nto switch it to another domain we would\\nhave to also fine tune it on new data\\nfrom that domain but that's not always\\nthe case with models like open ai's clip\\nfor performing each one of these tasks\\nin a zero shot setting now open ai's\\nclip is a multi-modal model that has\\nbeen pre-trained on a huge number of\\ntext and image Pairs and it essentially\\nworks by identifying text and image\\npairs that have a similar meaning and\\nplacing them within a similar Vector\\nspace so every text and every image gets\\nconverted into a vector and they are\\nplaced in a shared Vector space and the\\nvectors that appear close together they\\nhave a similar meaning now Clips very\\nbroad pre-training means that it can\\nperform very effectively across a lot of\\ndifferent domains it's seen a lot of\\ndata and so it has a good understanding\\nof all these different things and we can\\neven adjust the task being performed\\nwith just a few code changes we don't\\nactually have to is the model itself we\\njust adjust the code around the model\\nand that's very much thanks to Clips\\nfocus on sort of comparing these vectors\\nso for example for classification we\\ngive clip a list of our plus labels and\\nthen we pass in the images and we just\\nidentify within that space where those\\nimages are with respect to those plus\\nlabel vectors and which plus label is\\nthe most similar to our particular image\\nand then that is our prediction so that\\nmost similar plus label\\nthat's our predated class now for\\nobjects localization we apply a very\\nsimilar type of logic as before we\\ncreate a class label but unlike before\\nwe don't need the entire image into clip\\nto localize an object we have to break\\nthe image into patches we then pass a\\nwindow over all of those patches moving\\nacross the entire image left to right\\ntop to bottom and we generate a image\\nembedding for each of those windows and\\nthen we calculate the similarity between\\neach one of those windows embedded by\\nclip and the class label embedding\\nreturning a similarity score for every\\nsingle patch now after calculating the\\nsimilarity score for every single patch\\nwe use that to create almost like a map\\nof relevance across the entire image and\\nthen we can use that map to identify the\\nlocation of the object of interest and\\nfrom that we will get something that's\\nkind of like this so we have most of the\\nimage will be very dark and black that\\nmeans as the object of interest is not\\nin that space and then using that\\nlocalization map we can create a more\\ntraditional bounding box visualization\\nas well both of these visuals are\\ncapturing the same information we're\\njust displaying it in a different way\\nnow there's also other approaches to\\nthis so I recently hosted a talk with\\nwhat two two sets of people actually so\\nFederico Bianchi from Stanford's NLP\\ngroup and also Raphael pissoni and both\\nof those have worked on a Italian clip\\nproject and part of that was performing\\nobject localization now to do that they\\nuse a slightly different approach what\\nI'm going to demonstrate here and we can\\nthink of it as almost like the opposite\\nso whereas we slide a window over the\\nwhole image they slide a black patch\\nover the whole image which hides what is\\nbehind in that patch and then they feed\\nthe image into click and essentially as\\nyou slide the patch over the image you\\nare hiding a part of the image and\\ntherefore if this similarity score drops\\nwhen the patch is over a certain area\\nyou know that the object you're looking\\nfor is probably within that space and\\nthat's called the occlusion algorithm\\nnow moving on to object detection which\\nis like the last level in these three\\ntasks we will be identifying multiple\\nobjects now there's a very fine line\\nbetween object localization and object\\ndetection but you can simply think of it\\nas localization for multiple clusters\\nand multiple objects with our cat and\\nButterfly image we will be searching for\\ntwo objects a cat and a butterfly and\\nwith that we could draw a bounding box\\naround both of those objects and\\nessentially what we're doing now is\\nusing localization for a single object\\nbut then we're putting both of those\\ntogether in a loop in our code and we're\\nproducing this object detection process\\nnow we've covered the idea behind my\\nimage classification onto object\\nlocalization and object detection now\\nlet's have a look at how we actually\\nImplement all of this now before we move\\non to any classification localization or\\ndetection task we need to have some data\\nwe're going to use a small demo data set\\ncalled James Callum image text demo and\\nwe can download it like this so we're\\nusing hooking phase data sets here which\\nwe can pip install with Pip install\\ndata sets\\nand this is the day so it's very small\\nit's 21 text to image pairs okay one of\\nthose is the image you've already seen\\nthe cat with a butterfly landing on its\\nnose very curious how they got that\\nphoto now after you've downloaded that\\ndata set\\nwe're going to be using this image here\\nand what we want to do is not use the\\nimage file itself because at the moment\\nit's a it's a pill python image object\\nbut instead we need to convert it into a\\ncanister now we're going to be using pi\\ntorch later on so what I want to do here\\nis we're going to just transform the\\nimage into a tensor and we use toxin\\ntransforms use the typical pipeline tool\\nin computer vision and we just use tube\\ntensor okay and then we process our\\nimage through that Pipeline and then we\\ncan see that we get this okay so what\\nare these values here we have the height\\nof the image in pixels\\nthe width of the image in pixels and\\nthen also the three color channels red\\ngreen and blue that make up the image\\nnow we need a slightly different format\\nwhen we are processing everything one we\\nneed to add those patches and two we\\nneed to process it through a pie torch\\nmodel and we also need the batch\\ndimension for that so the first thing\\nwe're going to do is add the batch\\nDimension it's just a single image so we\\njust have one in there but we we need\\nthat anyway and then we come down to\\nhere so this is where we're going to\\nbreak the image into the patches okay\\neach patch is going to be 256 dimensions\\nin both height and width so the first\\nthing we do here is unfold and we get\\nthis here okay there's two five six and\\nthere's 20. now the 20 is the height of\\nthe image in these 256 pixel patches\\nand we can visualize that here\\nall right so now we have all these kind\\nof like slivers of the image that's just\\na vertical component of each patch\\nand we use unfold again but in this time\\nin the second Dimensions the targeting\\nwhat was this Dimension here and we also\\nget another 256 now we visualize that we\\nget our four patches\\nokay like this\\nnow if you just consider this here it's\\nlike if we look at this patch here it\\ndoesn't tell us anything about the image\\nright and even when we're over cats\\nthese patches are way too small to\\nactually tell us anything if clip is\\nprocessing a single patch at a time\\nit's probably not going to tell us\\nanything maybe it could tell us that\\nthere's some hair in this patch or that\\nthere's an eye in this patch but beyond\\nthat it's not going to be very useful so\\nrather than feeding single patches into\\nclip what we do is actually feed a\\nwindow a six by six patches or we can\\nmodify that value if we prefer and that\\njust gives us a big patch to pass over\\nto clip now the reason that we don't\\njust do that from the start we don't\\njust create these bigger patches to\\nbegin with is because when we're sliding\\nthrough the image we want to have some\\ndegree of overlap between each patch\\nokay so we create these smaller patches\\nand then what we can do is actually\\nslide across just one little patch at a\\ntime and we Define that using the stride\\nvariable so if we come down to here\\nwe have window we have stride\\nremove this\\nand here we go this is our code for\\ngoing through the whole image creating a\\npatch at every time step okay so we go\\nfor y and we go through the whole y-axis\\nand then within that we're going across\\nleft to right with each step and we\\ninitialize a empty big patch array so\\nthis is our like the full window\\nwe got the current batch so okay let's\\nsay we start at zero zero X zero y zero\\nwe go from zero to six and zero to six\\nhere\\nright so that gives us the very top left\\ncorner or window of the image and then\\nwe're literally going through and and\\njust go processing all of that and you\\ncan see that happening here as wine eggs\\nare increasing we're moving through that\\nimage and we're seeing each big patch\\nfrom our image okay sliding across with\\na single small little patch at a time so\\nthat we don't miss any important\\ninformation\\nnow this is how we're going to run\\nthrough the whole image but before we do\\nthat we actually need clip so let's go\\nahead and actually initialize clip\\nso to do that all we do is this so we're\\nusing hook and face Transformers which\\nis using pi torch in the in the back\\nthere so we need the clip processor\\nwhich is like a pre-processing pipeline\\nfor both text and images and then the\\nactual model itself okay so we some\\nmodel ID and we initialize both of those\\nthen what we want to do is move the\\nmodel to advice if possible all right so\\nwe can use CPU but if you have a Kudo\\nenabled GPU that will be here much\\nfaster so I'd recommend doing that if\\nyou can if not then you can use CPU it\\nwill be a bit slower but it will still\\nrun within a variable time frame so if\\nI'm running this on my Mac I am using a\\nCPU you can actually run this on MPS as\\nwell so you could change your device to\\nMPS if you have an MPS enabled Apple\\nsilicon device\\nso now returning to that process where\\nwe're going through each window within\\nthe image we're just going to add a\\nlittle bit more logic so we are\\nprocessing like we were before there's\\nnothing different here we're creating\\nthat big patch and then what we do is\\nprocess that big patch and process a\\ntext label okay so at the moment we're\\nlooking for a fluffy cat within this\\nimage so that is how we do this we're\\nreturning Pi torch tensors we also add\\npadding here as well for the text\\nalthough in this case I don't think we\\nneed it because we only have a single\\ntext item but we include that when we're\\nusing multiple text items later and then\\nwe calculate and retrieve the similarity\\nscore between them okay so if we pass\\nboth text and images through this\\nprocessor we'll pass both into our\\ninputs here and then we just calculate\\nthe or we extract the logits for each\\nimage and the item just converts that\\ninto a array of values or single value\\nand then here we have those scores so\\nwhat we're doing here is creating the\\nwhat I earlier called like the relevance\\nmap or localization map throughout the\\nwhole image so for every window that we\\ngo through we're adding this score to\\nevery single patch or little patch\\nwithin that window and what we're going\\nto do or what we're going to find when\\nwe do that is that some patches will\\nnaturally have a high score than others\\nbecause they are viewed more times right\\nso if you think about the top left patch\\nin the image that's only going to be\\nviewed once whereas patches in the\\nmiddle are going to be viewed many times\\nbecause we'll have a sliding window\\ngoing over there multiple times so what\\nwe also need to do is identify the\\nnumber of runs that we perform or number\\nof calculations that we perform within\\neach one of those patches the reason we\\ndo that is so that we can take the\\naverage for each score based on the\\nnumber of times that score has been\\ncapital related because here we're\\ntaking the total Awards scores and then\\nwe'll just take the average like so now\\nthe scores tensor is going to have a\\nvery smooth gradient of values from zero\\ncompletely irrelevant to one now if you\\nconsider that we've been going over\\nthese scores multiple times it means\\nthat the object of interest is kind of\\nlike faded out of the window like over\\nmultiple steps so that means that the\\nsimilarities score quite gradually faves\\nout as you go away from the object which\\nmeans that you don't really get very\\ngood localization if you use these\\nscores directly so what you what we need\\nto do is actually clip the lower scores\\ndown to zero so to do that what it is\\ncalculate the average of scores across\\nthe whole image we subtract that average\\nfrom the current scores what that will\\ndo is push 50 of the scores below zero\\nand then we click those scores so\\nanything below zero becomes zero and we\\ncan do this multiple times okay one time\\nis usually enough but you can do it\\nmultiple times to increase that effect\\nof making the edge of this detected or\\nlocalized area better defined and then\\nafter you've done that what we need to\\ndo is normalize those scores okay so we\\nmight have to do this a few times but\\neverything's probably going to be within\\nthe range of like zero to 0.5 or 0 to\\n0.2 so then we normalize those scores\\nbring them back within the range of zero\\nto one now to apply these scores to the\\npatches we need to align their tenses\\nbecause right now that they are not\\naligned okay for the scores we just we\\nhave like 20 by 13 tensor but for the\\npatches we have the the batch Dimension\\nthere we have the 20 by 13 which we do\\nwant but then we have the three color\\nchannels and the two five six for each\\num set of pixels within each patch so we\\nneed to adjust that a little bit so we\\nneed to First remove the batch Dimension\\nwe do that by squeezing out the zero\\nDimension which is a batch Dimension and\\nthen we permute the different dimensions\\nit's actually just moving them around in\\nour patches in order to align them\\nbetter with the score tensor dimensions\\nand then all we do is multiply the\\npatches by those scores that's pretty\\nstraightforward\\nthen we have to mute them again because\\nif we want to visualize everything needs\\nto be within a certain shape in order\\nfor us to visualize our matplotlib\\nso we come down and the first thing you\\ndo is just get y next here so Y and X\\nare the the patches\\nsee here this is y so the height of the\\nimage in patches and then 13 which is\\nthe width of the image in patches and we\\ncome down here and we can plot this okay\\nand we get this it's pretty nice visual\\nlocalizes the The Fluffy Cuts within\\nthat image now what's really interesting\\nis if we just search for a cat we\\nactually get a slightly different\\nlocalization because here you can see\\nit's kind of focusing a lot on the\\nfluffy part of the cat so if we just\\nsearch for a cat it would actually focus\\nmore on the head so we can really add\\nnuanced information to these prompts and\\nget a pretty nuanced response back\\nnow we can do the same for butterfly so\\nwe'll just throw all that code together\\nthis is just what we've done before we\\ninitialize scores and runs and we go\\nprocess all of that the only thing we\\nchange here is the prompt we change it\\nto a butterfly and if we go down and\\nwe're going to go down and down and\\nvisualize that we'll get this okay so\\nagain that's that's pretty cool we can\\nsee that it is identifying where in the\\nimage that butterfly actually is so that\\nis the object localization set\\nnow I want to have a look at object\\ndetection which is essentially just\\ntaking the object localization and and\\nwrapping some more code around it\\nin order to look at these multiple\\nobjects rather than just one but to do\\nthat we can't really visualize in the\\nsame way that we've done here we're\\ngoing to need a different type of\\nvisualization and that's where we have\\nthe bounding boxes so let's take a look\\nat how we would do that so using the I\\nthink the butterfly examples of\\nbutterfly scores that we just calculated\\nwe're going to look at where those\\nscores are higher than 0.5 now you can\\nadjust this threshold based on you know\\nwhat you find works best so we do this\\nand what we'll get is a array of true\\nand false values as to where the score\\nwas higher than 0.5 and not\\nand then we detect where the non-zero\\nvalues are in that array and what we do\\nis get a load of X and Y values here\\nso position three two we know that there\\nis a score that is higher than 0.5 and\\nwe get three and two here so three is\\nthe row of the non-zero value and 2 is a\\ncolumn of the non-zero value so at row\\nposition three and column two we know\\nthat there is a non-zero value or a\\nvalue or score that's higher than 0.5\\nour threshold\\nand put all that together we'll get\\nsomething looks kind of like this so we\\nalready we kind of see that\\nlocalization visual that we we just\\ncreated\\nand what we want to do is identify the\\nbounding box that's just kind of\\nsurrounding those values okay so we know\\nin terms of like a coordinate system we\\nwant one and three and four and ten to\\nbe included within that so what we do is\\nfind the corners from the detection\\narray or or set of\\ncoordinates that we got before from NP\\nnon-zero and what we do is we just take\\nthe minimum X and Y values and maximum X\\nand Y values and that will give us the\\ncorners of the box\\nthat's pretty simple to to calculate now\\nwhen we get the maximum value what we\\nwant to do is because we basically we\\nget in the position of the patch and the\\nposition of each patch we're essentially\\nidentifying the top left corner of each\\npatch so when we're looking at the\\nmaximum value we actually want not the\\nsolved patch but the end of the patch\\nokay so that's why we add that plus one\\nhere in order to get that\\nthe same for the x max value as well so\\nthat gives us a corner coordinates and\\nthen what we do is multiply those Corner\\ncoordinates by the patch size it's 256\\npixels and then we have the pixel\\npositions of each one of those Corners\\nbecause before we had the patch\\ncoordinates now we have the pixel\\ncoordinates which we can map directly\\nonto the original image so we can see\\nthe minimum values here so we have for x\\nand y two five six and a seven six eight\\nand what we want to do because we're\\ngoing to be using matplotlib patches and\\nmatplotlip patches expects the top left\\ncorner coordinates and the width and\\nheight of the bounding box that you want\\nto create so we calculate the width and\\nheight and that's pretty simple it's\\njust y Max minus y Min and X knives\\nminus X min\\nlook at these\\nand what we can do now is take the image\\nwe have to\\nreshape it a little bit so we have to\\nmove the three color channels Dimension\\nfrom the zeroth dimension to the final\\nDimension so we just do that here move\\naxis\\nand now we can plot that image okay so\\nwe showed that image with matplotlib\\nand then we create a rectangle patch\\nthis is our bounding box okay so we pass\\nX Min and Y Min that's the top left\\ncorner and then we also pass the width\\nand height of what the boundary box\\nshould be\\nand if we come down we get this visual\\nokay so that's our bounding box\\nvisualization and with that it's not\\nmuch further to create our object\\ndetection so let's have a look at how we\\ndo that now the logic for this is pretty\\nmuch just a loop over what we've already\\ndone so I've put together a load of\\nfunctions here which is essentially just\\nwhat we've already gone through getting\\npatches getting the the scores getting\\nthe the box and then the one thing that\\nis new here is this detect function okay\\nso we have detect that's going to get\\nthe the patches so it's going to take an\\nimage and it's going to split into those\\npatches that we created we're going to\\nconvert the image into a format for\\ndisplaying with matplotlib we did that\\nbefore and we also initialized that plot\\nand add our image to that plot and then\\nwhat we do is we have for Loop and this\\nfor Loop goes through the image\\nlocalization steps and bounding box\\nsteps that we just went through\\njust multiple times okay so we have\\nmultiple primes and we want to do it\\nmultiple times so we calculate our\\nsimilarity scores based on a specific\\nprompt\\nfor all of our image Patches from that\\nwe get our our scores in that patch\\ntensor format that we saw before\\nand then what we do is we want to get\\nthe box based on a particular threshold\\nso 0.5 like we used before you can see\\nover there we have our patch size we\\njust need to pass to that for a\\ncalculation of the or for the conversion\\nand we have our patch size which we\\npassed to that for the conversion from\\npatch\\npixel from patch coordinates to pixel\\ncoordinates now we also have our scores\\nand that will return the minimum X and Y\\ncoordinates and also width and height of\\nthe box\\nwe create the bounding box\\nnow we add that to the axis okay so now\\nlet's visualize all this see what we get\\nso here I've used a slightly smaller\\nwindow size before using six just to\\npoint out that you can change it and\\ndepending on your image it may be better\\nto use a smaller or larger window\\nand you can see so what we're doing here\\nwe've got a cat and a butterfly and you\\ncan see that we get we get a butterfly\\nhere and we get the cat here okay it's\\npretty cool and like I said with clip we\\ncan apply this object detection without\\nfine tuning all we need to do is change\\nthese prompts here\\nokay so it's it's really straightforward\\nto modify this and move it to a new\\ndomain okay so that's it for this\\nwalkthrough of object localization and\\nobject detection with clip as I said I\\nthink zero shot object localization\\ndetection and even classification opens\\nthe doors to a lot of projects and use\\ncases that were just not accessible\\nbefore because time and capital\\nconstraints and now we can just use clip\\nand get pretty impressive results very\\nquickly all it requires is a bit of code\\nchanging here and there now I think clip\\nis one part of a trend in multimodality\\nthat is kind of creating a more\\naccessible ml that is less brittle like\\nmodels were in the past that required a\\nlot of fine tuning just to adapt to a\\nslightly different domain and just more\\ngenerally applicable which I thing is\\nreally exciting and I'm I'm it's really\\ncool to see this sort of thing actually\\nbeing used and to actually use it and\\njust see how easy it is to use clip for\\nso many different use cases and it work\\nlike incredibly easily so that's it for\\nthis video I hope it has been useful\\nso thank you very much for watching and\\nI will see you again in the next one bye\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Initializes YoutubeTranscriptReader to fetch and load transcripts for provided YouTube video links.\n",
        "from llama_hub.youtube_transcript import YoutubeTranscriptReader\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=i3OYlaoj-BM'])\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads transcript for the specified YouTube video using YoutubeTranscriptReader.\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=ZA-tUyM_y7s'],languages=['en'])\n",
        "documents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzYvrSb1uPYz",
        "outputId": "0f4e9e0b-c2ed-4c0f-8fca-7259fbd95b0f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='ee070ba6-e36b-4bb1-ac56-dbda3c493086', embedding=None, metadata={'video_id': 'ZA-tUyM_y7s'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='15a13c38db3d8cbacc71413d9a44f41a9b7f998423664787da2f949067195bac', text=\"[SQUEAKING]\\n[RUSTLING]\\n[CLICKING]\\nJASON KU: Good\\nmorning, everybody.\\nSTUDENT: Morning--\\nJASON KU: My name's Jason Ku.\\nI'm going to be teaching\\nthis class in Introduction\\nto Algorithms with two\\nother instructors here--\\nfaculty in the department--\\nEric Demaine and Justin Solomon.\\nThey're excellent\\npeople, and so they\\nwill be working on teaching\\nthis class with me.\\nI will be teaching\\nthe first lecture,\\nand we'll have\\neach of them teach\\none of the next two lectures,\\nand then we'll go from there.\\nThis is Intro to Algorithms.\\nOK, so we're going to start\\ntalking about this course\\ncontent now.\\nWhat is this course about?\\nIt's about algorithms--\\nintroduction to algorithms.\\nReally what the\\ncourse is about is\\nteaching you to solve\\ncomputational problems.\\nBut it's more than that.\\nIt's not just about\\nteaching you to solve\\ncomputational problems.\\nGoal 1-- solve\\ncomputational problems.\\nBut it's more than that.\\nIt's also about communicating\\nthose solutions to others\\nand being able to communicate\\nthat your way of solving\\nthe problem is\\ncorrect and efficient.\\nSo it's about two more things--\\nprove correctness,\\nargue efficiency,\\nand in general, it's\\nabout communication--\\nI can't spell, by the way--\\ncommunication of these ideas.\\nAnd you'll find that, over\\nthe course of this class,\\nyou'll be doing a lot\\nmore writing than you do\\nin a lot of your other courses.\\nIt really should maybe\\nbe a CI kind of class,\\nbecause you'll be doing a\\nlot more writing than you\\nwill be coding, for sure.\\nOf course, solving the\\ncomputational problem\\nis important, but\\nreally, the thing\\nthat you're getting out of this\\nclass and other theory classes\\nthat you're not getting in\\nother classes in this department\\nis that we really\\nconcentrate on being\\nable to prove that the\\nthings you're doing\\nare correct and better\\nthan other things,\\nand being able to communicate\\nthose ideas to others, and not\\njust to a computer--\\nto other people, convince\\nthem that it's correct.\\nOK, so that's what\\nthis class is about.\\nSo what do I mean when I say\\nsolve a computational problem?\\nWhat is a problem?\\nWhat is an algorithm?\\nPeople make fun of me because\\nI start with this question,\\nbut anyone want to\\nanswer that question?\\nNo?\\nWhat's a problem,\\ncomputationally?\\nNo?\\nOK, so it's not such\\na stupid question.\\nYeah?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: Something\\nyou want to compute--\\nOK, yes, that's true.\\nRight.\\nBut a little bit more\\nabstractly, what I'm going to\\nthink of a computational\\nproblem being--\\nand this is where\\nyour prerequisite\\nin discrete mathematics\\nshould come in--\\na problem is-- you've\\ngot a set of inputs.\\nMaybe I have one, two, three,\\nfour, five possible inputs\\nI could have to my algorithm.\\nThen I have a space of outputs.\\nI don't know.\\nMaybe I have more of\\nthem than I do inputs,\\nbut these are the possible\\noutputs to my problem.\\nAnd what a problem is\\nis a binary relation\\nbetween these\\ninputs and outputs.\\nEssentially, for each input, I\\nspecify which of these outputs\\nis correct.\\nIt doesn't necessarily\\nhave to be one.\\nIf I say, give me the index in\\nan array containing the value\\n5, there could be multiple\\n5's in that array,\\nand so any of those\\nindices would be correct.\\nSo maybe this guy maps to that\\noutput, and maybe this guy maps\\nto--\\nI don't know-- two\\nor three outputs.\\nThis input goes to one, two--\\nI don't know.\\nThere's some kind\\nof mapping here.\\nThese edges represent\\na binary relation,\\nand it's kind of a\\ngraph, a bipartite graph\\nbetween these\\ninputs and outputs.\\nAnd these are specifying\\nwhich of these outputs\\nare correct for these inputs.\\nThat's really the\\nformal definition\\nof what a problem is.\\nNow, generally, if\\nI have a problem--\\na computational\\nproblem, I'm not going\\nto specify the problem to you\\nby saying, OK, for input 1,\\nthe correct answer is\\n0, and for input 2,\\nthe correct answer's 3,\\nand so on and so forth.\\nThat would take forever, right?\\nUsually what we do\\nwhen defining a problem\\nis specify some kind of\\npredicate, saying that,\\noh, we can check--\\nif I give you an\\ninput and an output,\\nI can check whether that\\noutput is correct or not.\\nThat's usually how we\\ndefine a problem is,\\nif I am checking for whether\\nthis index contains a 5,\\nI can just go to that array,\\nlook at index 5, and--\\nor the index you gave me,\\nand see if it equals 5.\\nSo usually, we're putting\\nit in terms of predicates\\nbecause, in general,\\nwe don't really\\nwant to talk about small\\ninstances of problems.\\nSo let's say I had\\nthe problem of,\\namong the students in this\\nclassroom, do any pair of you\\nhave the same birthday?\\nAll right, well, probably, if\\nthere's more than 365 of you,\\nthe answer is yes.\\nRight?\\nBy what?\\nPigeonhole\\nprinciple-- two of you\\nmust have the same birthday.\\nSo let's generalize it\\na little bit, say that--\\nI don't know--\\nI need a bigger space of\\nbirthdays for this question\\nto be interesting.\\nMaybe I tack on the year.\\nMaybe I tack on the\\nhour that you were born.\\nAnd that's a bigger\\nspace of inputs,\\nand I wouldn't necessarily\\nexpect that two of you\\nwould be born in the\\nsame year on the same day\\nin the same hour.\\nThat would be a\\nlittle less likely.\\nIn fact, as long as that\\nspace is larger than something\\nlike the square of the\\nnumber of you, then\\nI'm less likely than even\\nto have a pair of you.\\nThat's a birthday problem\\nyou may have seen in 042,\\npotentially.\\nBut in general, I don't--\\nI'm not going to mess with\\nprobability so much here.\\nI want a deterministic\\nalgorithm, right away\\nof checking whether two of\\nyou have the same birth time,\\nlet's say.\\nOK, so in general,\\nin this class,\\nwe're not going to\\nconcentrate on inputs such as,\\nis there a pair of\\nyou in this class\\nthat have the same birthday?\\nThat's kind of boring.\\nI could do a lot of\\ndifferent things,\\nbut what we do in\\nthis class-- this\\nis for a fixed classroom of you.\\nI want to make algorithms that\\nare general to any classroom--\\nto go to your recitation.\\nI want an algorithm that will\\napply to your recitation.\\nI want an algorithm that not\\nonly applies to this classroom,\\nbut also the machine\\nlearning class before you.\\nI want an algorithm\\nthat can change its--\\nit can accept an\\narbitrarily sized input.\\nHere we have a class of\\nmaybe 300, 400 students,\\nbut I want my algorithm to\\nwork for a billion students.\\nMaybe I'm trying\\nto check if there's\\na match of something in\\nthe Facebook database\\nor something like that.\\nSo in general, we are looking\\nfor general problems that\\nhave arbitrarily sized inputs.\\nSo these inputs could\\ngrow very large,\\nbut we want kind of a\\nfixed size algorithm\\nto solve those problems.\\nSo what is an algorithm, then?\\nI really can't spell--\\ntold you.\\nI didn't lie to you.\\nSo an algorithm is a little\\ndifferent than a problem.\\nA problem specification--\\nI can tell you what\\nthis graph looks like.\\nAn algorithm is really--\\nI don't know what\\nthe outputs are.\\nI don't know what\\nthese edges are.\\nBut I want a fixed size\\nmachine or procedure\\nthat, if I give it an input,\\nit will generate an output.\\nAnd if it generates\\nan output, it better\\nbe one of these correct outputs.\\nSo if I have an algorithm\\nthat takes in this input,\\nI really want it to\\noutput this output,\\nor else it's not a\\ncorrect algorithm.\\nSimilarly, for this one, it\\ncould output any of these three\\noutputs, but if it outputs\\nthis guy for this input,\\nthat would not be a\\ncorrect algorithm.\\nAnd so generally, what we want\\nis an algorithm is a function.\\nIt takes inputs to outputs.\\nAn algorithm is some\\nkind of function\\nthat takes these inputs,\\nmaps it to a single output,\\nand that output better be\\ncorrect based on our problem.\\nSo that's what our algorithm is.\\nIt solves the\\nproblem if it returns\\na correct output for\\nevery problem input\\nthat is in our domain.\\nDoes anyone have a\\npossible algorithm\\nfor checking whether\\nany two of you\\nhave the same birth time,\\nas specified before?\\nI'm going to let\\nsomeone else have a try.\\nSure.\\nSTUDENT: Just ask everyone\\none by one, and every time\\n[INAUDIBLE]\\nJASON KU: Great-- so what\\nyour colleague has said\\nis a great algorithm.\\nEssentially, what\\nit's going to do\\nis I'm going to put\\nyou guys in some order,\\nI'm going to give\\nyou each of you\\na number, one through however\\nmany number of students there\\nare in this class.\\nAnd I'm going to\\ninterview you one by one.\\nI'm going to say,\\nwhat's your birthday?\\nAnd I'm going to write it down.\\nI'm going to put it in\\nsome kind of record.\\nAnd then, as I keep\\ninterviewing you,\\nI'm going to find\\nout your birthday.\\nI'm going to check the record.\\nI'm going to look through all\\nthe birthdays in the record.\\nIf I find a match,\\nthen I return, yay--\\nI found a pair-- and I can stop.\\nOtherwise, if I get\\nthrough the record list,\\nI don't-- and I\\ndon't find a match,\\nI just stick you at\\nthe end of the record--\\nI add you to the\\nrecord, and then I\\nmove on to the next person.\\nI keep doing this.\\nOK, so that's a\\nproposed algorithm\\nfor this birthday problem.\\nFor birthday problem,\\nwhat's the algorithm here?\\nMaintain a record.\\nInterview students\\nin some order.\\nAnd what does interviewing\\na student mean?\\nIt means two things.\\nIt means check if\\nbirthday in record.\\nAnd if it is, return a pair.\\nSo return pair.\\nOtherwise, add a new\\nstudent to record.\\nAnd then, at the very end,\\nif I go through everybody\\nand I haven't found\\na match yet, I'm\\ngoing to return\\nthat there is none.\\nOK, so that's a statement\\nof an algorithm.\\nThat's kind of the\\nlevel of description\\nthat we'll be looking for you\\nin the three parts of this--\\ntheory questions that we ask\\nyou on your problem sets.\\nIt's a verbal description\\nin words that--\\nit's maybe not enough for a\\ncomputer to know what to do,\\nbut if you said this algorithm\\nto any of your friends\\nin this class, right they\\nwould at least understand\\nwhat it is that you're doing.\\nYeah?\\nSTUDENT: Does an algorithm\\nhave to be a pure function\\nin a mathematical sense?\\nJASON KU: Does an algorithm\\nhave to be a pure function\\nin a mathematical sense?\\nAs in it needs to map\\nto a single output?\\nSTUDENT: As in it can't\\nmodify some external state.\\nIt can't take in state\\nand it can't do I/O.\\nJASON KU: So we're\\ntalking about kind\\nof a functional programming\\ndefinition of a function.\\nI am talking about\\nthe mathematical--\\nI have a binary\\nrelation, and this thing\\nhas an output for every\\ninput, and there is exactly\\none output to every input.\\nThat's the mathematical\\ndefinition of function\\nthat I'm using for when\\nI'm defining an algorithm.\\nYeah?\\nSTUDENT: Basically, is\\nan algorithm like a plan?\\nJASON KU: Yeah.\\nAn algorithm's a\\nprocedure that somehow--\\nI can do whatever\\nI want, but I have\\nto take one of these inputs and\\nI have to produce an output.\\nAnd at the end, it\\nbetter be correct.\\nSo it's just a procedure.\\nYou can think of it\\nas like a recipe.\\nIt's just some\\nkind of procedure.\\nIt's a sequence of things\\nthat you should do,\\nand then, at the end, you\\nwill return an output.\\nS here's a possible algorithm\\nfor solving this birthday\\nproblem.\\nNow, I've given you--\\nwhat I argue to you, or\\nI'm asserting to you,\\nis a solution to this\\nbirthday problem.\\nAnd maybe you guys\\nagree with me,\\nand maybe some of you don't.\\nSo how do I convince you\\nthat this is correct?\\nIf I was just running\\nthis algorithm on, say,\\nthe four students in\\nthe front row here,\\nI could argue it\\npretty well to you.\\nI could assign these\\nfor people birthdays\\nin various combinations\\nof either their--\\nnone of them have the same\\nbirthday, some two of them\\nhave the same birthday.\\nI could try all\\npossibilities, and I\\ncould go through lots of\\ndifferent possibilities\\nand I need to check that\\nthis algorithm returns\\nthe right answer\\nin all such cases.\\nBut when I have--\\nI don't know--\\n300 of you, that's\\ngoing to be a little bit\\nmore difficult to argue.\\nAnd so if I want to argue\\nsomething is correct in--\\nI want to prove something to you\\nfor some large value, what kind\\nof technique do I use\\nto prove such things?\\nYeah?\\nInduction, right?\\nAnd in general, what we do\\nin this class, what we do\\nis-- as a computer scientist is\\nwe write a constant sized piece\\nof code that can take on any\\narbitrarily large size input.\\nIf the input can be arbitrarily\\nlarge, but our code is small,\\nthen that code needs\\nto loop, or recurse,\\nor repeat some of\\nthese lines of code\\nin order to just\\nread that output.\\nAnd so that's another way you\\ncan arrive at this conclusion,\\nthat we're going\\nto probably need\\nto use recursion, induction.\\nAnd that's part\\nof the reason why\\nwe ask you to take\\na course on proofs,\\nand inductive reasoning,\\nand discrete mathematics\\nbefore this class.\\nOK, so how do we prove\\nthat this thing is correct?\\nWe got to use induction.\\nSo how can we set\\nup this induction?\\nWhat do I need for\\nan inductive proof?\\nSure.\\nSTUDENT: [INAUDIBLE]\\nJASON KU: Base case--\\nwe need a base case.\\nWe need some kind\\nof a predicate.\\nYeah, but we need\\nsome kind of statement\\nof a hypothesis of something\\nthat should be maintained.\\nAnd then we need to have an\\ninductive step, which basically\\nsays I take a small\\nvalue of this thing,\\nI use the inductive\\nhypothesis, and I argue it\\nfor a larger value of\\nmy well-ordered set\\nthat I'm inducting over.\\nFor this algorithm,\\nif we're going\\nto try to prove correctness,\\nwhat I'm going to do\\nis I'm going to--\\nwhat do I want to\\nprove for this thing?\\nThat, at the end of\\ninterviewing all of you,\\nthat my algorithm has\\neither already-- it\\nhas returned with\\na pair that match,\\nor if we're in a\\ncase where there\\nwasn't a pair somewhere in my\\nset, that it returned none.\\nRight?\\nThat would be correct.\\nSo how can I\\ngeneralize that concept\\nto make it something\\nI can induct on?\\nWhat I'm going to do\\nis I'm going to say--\\nlet's say, after I've\\ninterviewed the first K\\nstudents, if there was a match\\nin those first K students,\\nI want to be sure that\\nI've returned a pair--\\nbecause if, after I\\ninterview all of you,\\nI've maintained\\nthat property, then\\nI'll be sure, at the\\nend of the process,\\nI will have returned\\na pair, if one exists.\\nSo here's going to be\\nmy inductive hypothesis.\\nIf first K students\\ncontain a match,\\nalgorithm returns a match\\nbefore interviewing, say,\\nstudent K plus 1.\\nSo that's going to be\\nmy inductive hypothesis.\\nNow, if there's n\\nstudents in this class,\\nand at the end of\\nmy thing, I'm trying\\nto interview a student n plus\\n1-- oh, student n plus 1's\\nnot there.\\nIf I have maintained this,\\nthen, if I replace K with n,\\nthen I will have\\nreturned a match\\nbefore interviewing\\nthe last student--\\nwhen I have no\\nmore students left.\\nAnd then this algorithm\\nreturns none, as it should.\\nOK, so this inductive hypothesis\\nsets up a nice variable\\nto induct on.\\nThis K I can have\\nincreasing, up to n,\\nstarting at some base case.\\nSo what's my base case here?\\nMy base case is--\\nthe easiest thing I can do--\\nsure-- 2?\\nThat's an easy thing I could do.\\nI could check those\\npossibilities,\\nbut there's an even\\neasier base case.\\nYeah?\\nThere's an even easier\\nbase case than 1.\\nSTUDENT: 0--\\nJASON KU: 0, right?\\nAfter interviewing 0 students,\\nI haven't done any work, right?\\nCertainly, the first\\n0 can't have a match.\\nThis inductive\\nhypothesis this is true\\njust because this initial\\npredicate is false.\\nSo I can say, base case 0--\\ncheck.\\nDefinitely, this\\npredicate holds for that.\\nOK.\\nNow we got to go for\\nthe meat of this thing.\\nAssume the inductive\\nhypothesis true\\nfor K equals, say, some K prime.\\nAnd we're considering\\nK prime plus 1.\\nThen we have two cases.\\nOne of the nice\\nthings about abduction\\nis that it isolates our problem\\nto not consider everything\\nall at once, but break it\\ndown into a smaller interface\\nso I can do less\\nwork at each step.\\nSo there are two cases.\\nEither the first K\\nalready had a match--\\nin which case, by our\\ninductive hypothesis,\\nwe've already returned\\na correct answer.\\nThe other case is the--\\nit doesn't have a match, and\\nwe interview the K plus 1th\\nstudent--\\nthe K prime plus 1th student.\\nIf there is a match in the\\nfirst K prime plus 1 students,\\nthen it will include K plus--\\nthe student K prime plus\\n1, because otherwise,\\nthere would have been a match\\nin the things before it.\\nSo there are two cases.\\nIf K contains match, K prime.\\nIf first K contains match--\\nalready returned by induction.\\nElse, if K prime plus 1\\nstudent's contains match,\\nthe algorithm checks all\\nof the possibilities--\\nK prime checks\\nagainst all students,\\nessentially by brute force.\\nIt's a case analysis.\\nI check all of\\nthe possibilities.\\nCheck if birthday is in record--\\nI haven't told you\\nhow to do that yet,\\nbut if I'm able to\\ndo that, I'm going\\nto check if it's in the record.\\nIf it's in the record,\\nthen there will be a match,\\nand I can return it.\\nOtherwise, I have-- re-establish\\nthe inductive hypothesis\\nfor the K prime plus 1 students.\\nDoes that makes sense, guys?\\nYeah.\\nOK, so that's how we\\nprove correctness.\\nThis is a little bit\\nmore formal than we\\nwould ask you to do in\\nthis class all the time,\\nbut it's definitely sufficient\\nfor the levels of arguments\\nthat we will ask you to do.\\nThe bar that we're\\nusually trying to set\\nis, if you communicated\\nto someone else taking\\nthis class what\\nyour algorithm was,\\nthey would be able to code it\\nup and tell a stupid computer\\nhow to do that thing.\\nAny questions on induction?\\nYou're going to be using\\nit throughout this class,\\nand so if you are unfamiliar\\nwith this line of argument,\\nthen you should go\\nreview some of that.\\nThat would be good.\\nOK, so that's correctness,\\nbeing able to communicate\\nthat the problem--\\nthe algorithm we\\nstated was correct.\\nNow we want to argue\\nthat it's efficient.\\nWhat does efficiency mean?\\nEfficiency just means\\nnot only how fast\\ndoes this algorithm run,\\nbut how fast does it\\ncompare to other possible ways\\nof approaching this problem?\\nSo how could we measure\\nhow fast an algorithm runs?\\nThis is kind of\\na silly question.\\nYeah?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: Yeah.\\nWell, just record the time\\nit takes for a computer\\nto do this thing.\\nNow, there's a problem with\\njust coding up an algorithm,\\ntelling a computer what to do,\\nand timing how long it takes.\\nWhy?\\nYeah?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: It would depend on\\nthe size of your data set.\\nOK, we expect that, but\\nthere's a bigger problem there.\\nYeah?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: It depends on the\\nstrength of your computer.\\nSo I would expect that, if\\nI had a watch calculator\\nand I programmed\\nit to do something,\\nthat might take a lot longer to\\nsolve a problem than if I asked\\nIBM's research computer to\\nsolve the same problem using\\nthe same algorithm,\\neven with the same code,\\nbecause its underlying\\noperations are much faster.\\nHow it runs is much faster.\\nSo I don't want to\\ncount how long it\\nwould take on a real machine.\\nI want to abstract the\\ntime it takes the machine\\nto do stuff out of the picture.\\nWhat I want to say\\nis, let's assume\\nthat each kind of fundamental\\noperation that the computer can\\ndo takes some fixed\\namount of time.\\nHow many of those kinds\\nof fixed operations\\ndoes the algorithm\\nneed to perform to be\\nable to solve this problem?\\nSo here we don't measure time.\\nInstead, count\\nfundamental operations.\\nOK?\\nWe'll get to what some of\\nthose fundamental operations\\nare in a second,\\nbut the idea is we\\nwant a measure of how well\\nan algorithm performs,\\nnot necessarily\\nan implementation\\nof that algorithm--\\nkind of an abstract notion of\\nhow well this algorithm does.\\nAnd so what we're going to use\\nto measure time or efficiency\\nis something called\\nasymptotic analysis.\\nAnyone here understand what\\nasymptotic analysis is?\\nProbably, since it's in both of\\nyour prerequisites, I think--\\nbut we will go through\\na formal definition\\nof asymptotic notation\\nin recitation tomorrow,\\nand you'll get a lot of\\npractice in comparing functions\\nusing an asymptotic analysis.\\nBut just to give you\\nan idea, the idea\\nhere is we don't measure time.\\nWe instead measure ops.\\nAnd like your colleague\\nover here was saying before,\\nwe expect performance--\\nI'm going to use performance,\\ninstead of time here--\\nwe expect that to depend\\non size of our input.\\nIf we're trying to\\nrun an algorithm\\nto find a birthday\\nin this section,\\nwe expect the algorithm to run\\nin a shorter amount of time\\nthan if I were to run the\\nalgorithm on all of you.\\nSo we expect it to\\nperform differently,\\ndepending on the\\nsize of the input,\\nand how differently is\\nhow we measure performance\\nrelative to that input.\\nUsually we use n as a variable\\nfor what the size of our input\\nis, but that's not\\nalways the case.\\nSo for example, if we have\\nan array that I give you--\\nan n-by-n array, that--\\nwe're going to say n,\\nbut what's the\\nsize of our input?\\nHow much information do\\nI need to convey to you\\nto give you that information?\\nIt's n squared.\\nSo that's the size of our\\ninput in that context.\\nOr if I give you a graph, it's\\nusually the number of vertices\\nplus the number of edges.\\nThat's how big--\\nhow much space I\\nwould need to convey to you\\nthat graph, that information.\\nWe compare how fast an\\nalgorithm is with respect\\nto the size of the input.\\nWe'll use the\\nasymptotic notation.\\nWe have big O notation, which\\ncorresponds to upper bounds.\\nWe will have omega, which\\ncorresponds to lower bounds.\\nAnd we have theta, which\\ncorresponds to both.\\nThis thing is tight.\\nIt is bounded from\\nabove and below\\nby a function of this form.\\nWe have a couple\\nof common functions\\nthat relate an\\nalgorithm's input size\\nto its performance, some things\\nthat we saw all the time.\\nCan anyone give\\nme some of those?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: Say again.\\nSTUDENT: [INAUDIBLE]\\nJASON KU: Sorry.\\nSorry.\\nI'm not asking\\nthis question well,\\nbut has anyone heard\\nof a linear algorithm--\\na linear time algorithm?\\nThat's basically saying that the\\nrunning time of my algorithm--\\nperformance of my algorithm\\nis linear with respect\\nto the size of my input.\\nRight?\\nYeah?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: Say again.\\nSTUDENT: Like putting\\nsomething in a list--\\nJASON KU: Like putting\\nsomething in a list--\\nOK.\\nThere's a lot\\nbehind that question\\nthat we'll go into\\nlater this week.\\nBut that's an example of,\\nif I do it in a silly way,\\nI stick something in\\nthe middle of a list\\nand I have to move everything.\\nThat's an operation that\\ncould take linear time.\\nSo linear time is\\na type of function.\\nWe've got a number of these.\\nI'm going to start\\nwith this one.\\nDoes anyone know this one is?\\nConstant time-- basically, no\\nmatter how I change the input,\\nthe amount of time\\nthis running time--\\nthe performance of my\\nalgorithm takes, it\\ndoesn't really depend on that.\\nThe next one up is\\nsomething like this.\\nThis is logarithmic time.\\nWe have data n, which\\nis linear, and log n.\\nSometimes we call\\nthis log linear,\\nbut we usually just say n log n.\\nWe have a quadratic\\nrunning time.\\nIn general, if I have a\\nconstant power up here,\\nit's n to the c\\nfor some constant.\\nThis is what we call\\npolynomial time,\\nas long as c is some constant.\\nAnd this right here is\\nwhat we mean by efficient,\\nin this class, usually.\\nIn other classes, when\\nyou have big data sets,\\nmaybe this is efficient.\\nBut in this class, generally\\nwhat we mean is polynomial.\\nAnd as you get down\\nthis thing, things\\nare more and more efficient.\\nThere's one class I'm going to\\ntalk to you about over here,\\nwhich is something like--\\nlet's do this-- 2 to the\\ntheta of n, exponential time.\\nThis is some constant to\\na function of n that's,\\nlet's say, super linear,\\nthat's going to be pretty bad.\\nWhy is it pretty bad?\\nIf I were to plot some of these\\nthings as a function of n--\\nlet's say I plot values of up\\nto 1,000 on my n scale here.\\nWhat does constant look like?\\nMaybe this is 1,000 up here.\\nWhat does a constant look like?\\nLooks like a line--\\nit looks like a line\\nover here somewhere.\\nIt could be as high as\\nI want, but eventually,\\nanything that's an\\nincreasing function\\nwill get bigger than this.\\nAnd on this scale,\\nif I use log base\\n2 or some reasonable\\nsmall constant,\\nwhat does log look like?\\nWell, let's do an easier one.\\nWhat does linear look like?\\nYeah, this-- that's what I\\nsaw what a lot of you doing.\\nThat's linear.\\nThat's the kind of base that\\nwe're comparing everything\\nagainst.\\nWhat does log look like?\\nLike this-- OK,\\nbut at this scale,\\nreally, it's much closer\\nto constant than linear.\\nAnd actually, as n gets\\nmuch, much larger this almost\\nlooks like a straight line.\\nIt almost looks like a constant.\\nSo log is almost just\\nas good as constant.\\nWhat does exponential look like?\\nIt's the exact\\ninverse of this thing.\\nIt's almost an exact\\nstraight line going up.\\nSo this is crap.\\nThis is really good.\\nAlmost anything in this region\\nover here is better right.\\nAt least I'm gaining something.\\nI'm able to not go up too high\\nrelative to my input size.\\nSo quadratic-- I don't know--\\nis something like this,\\nand n log n is\\nsomething like this.\\nn log n, after a\\nlong time, really\\nstarts just looking linear\\nwith a constant multiplied\\nin front of it.\\nOK, so these things\\ngood, that thing bad--\\nOK?\\nThat's what that's\\ntrying to convey.\\nAll right, so how do\\nwe measure these things\\nif I don't know what my\\nfundamental operations are\\nthat my computer can use?\\nSo we need to define some\\nkind of model of computation\\nfor what our computer is\\nallowed to do in constant time,\\nin a fixed amount of time.\\nIn general, what we use\\nin this class is a machine\\ncalled a word RAM, which we use\\nfor its theoretical brevity.\\nWord RAM is kind\\nof a loaded term.\\nWhat do these things mean?\\nDoes someone know\\nwhat RAM means?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: Random access memory--\\nit means that I can randomly\\naccess different places\\nin memory in constant time.\\nThat's the assumption\\nof random access memory.\\nBasically, what our\\nmodel of a computer is\\nyou have memory,\\nwhich is essentially\\njust a string of bits.\\nIt's just a bunch\\nof 1's and 0's.\\nAnd we have a computer, like\\na CPU, which is really small.\\nIt can basically hold a\\nsmall amount of information,\\nbut it can change\\nthat information.\\nIt can operate on\\nthat information,\\nand it also has instructions\\nto randomly access\\ndifferent places in memory,\\nbring it into the CPU,\\nact on it, and read it back.\\nDoes that makes sense?\\nBut in general, we\\ndon't have an address\\nfor every bit in memory,\\nevery 0 and 1 in memory.\\nDoes anyone know how modern\\ncomputers are addressed?\\nYeah?\\nSTUDENT: [INAUDIBLE]\\nJASON KU: OK, so we're\\ngoing to get there.\\nActually, what a modern\\ncomputer is addressed in\\nis bytes, collections of 8 bits.\\nSo there's an address\\nI have for every 8 bits\\nin memory-- consecutive\\n8 bits in memory.\\nAnd so if I want to pull\\nsomething in into the CPU,\\nI give it an address.\\nIt'll take some chunk, and bring\\nit into the CPU, operate on it,\\nand spit it back.\\nHow big is that chunk?\\nThis goes to the answer that\\nyou were asking, which--\\nor saying, which is it's some\\nsequence of some fixed number\\nof bits, which we call a word.\\nA word is how big of\\na chunk that the CPU\\ncan take in from memory\\nat a time and operate on.\\nIn your computers, how\\nbig is that word size?\\n64 bits-- that's how much\\nI can operate on at a time.\\nWhen I was growing up,\\nwhen I was your age,\\nmy word size was 32 bits.\\nAnd that actually was a\\nproblem for my computer,\\nbecause in order for\\nme to be able to read\\nto address in\\nmemory, I need to be\\nable to store that address\\nin my CPU, in a word.\\nBut if I have 32 bits, how\\nmany different addresses can\\nI address?\\nI have a limitation on the\\nmemory addresses I can address,\\nright?\\nSo how many different\\nmemory addresses\\ncan I address with 32 bits?\\n2 to the 32, right?\\nThat makes sense.\\nWell, if you do that calculation\\nout, how big of a hard disk\\ncan I have to access?\\nIt's about 4 gigabytes.\\nSo in my day, all\\nthe hard drives\\nwere limited to being\\npartitioned-- even if you\\nhad a bigger than 4\\ngigabyte hard drive,\\nI had to partition it into\\nthese 4 gigabyte chunks, which\\nthe computer could\\nthen read onto.\\nThat was very\\nlimiting, actually.\\nThat's a restriction.\\nWith 64 bits, what's\\nmy limitation on memory\\nthat I can address--\\nbyte addressable?\\nTurns out to be something\\nlike 20 exabytes--\\nto put this in\\ncontext, all data that\\nGoogle stores on\\ntheir servers, on all\\ndrives throughout the world--\\nit's about 10.\\nSo we're not going to run out\\nof this limitation very soon.\\nSo what do we got\\nwe've got a CPU.\\nIt can address memory.\\nWhat are the operations\\nI can do in this CPU?\\nGenerally, I have\\nbinary operations.\\nI can compare to\\nwords in memory,\\nand I can either do integer\\narithmetic, logical operations,\\nbitwise operations--\\nbut we're not going to use\\nthose so much in this class.\\nAnd I can write and write\\nfrom an address in memory,\\na word in constant time.\\nThose are the\\noperations that I have\\navailable to me on most CPUs.\\nSome CPUs give you a\\nlittle bit more power,\\nbut this is generally what we\\nanalyze algorithms with respect\\nto.\\nOK?\\nBut you'll notice\\nthat my CPU is only\\nbuilt to operate on a constant\\namount of information at once--\\ngenerally, two words in memory.\\nAn operation produces a\\nthird one, and I spit it out.\\nIt takes a constant\\namount of time\\nto operate on a constant\\namount of memory.\\nIf I want to operate on a\\nlinear amount of memory--\\nn things-- how long\\nis that going to take?\\nIf I just want to read\\neverything in that thing,\\nit's going to take\\nme linear time,\\nbecause I have to read\\nevery part of that thing.\\nOK, so in general,\\nwhat we're going\\nto do for the first half\\nof this class mostly--\\nfirst eight lectures, anyway--\\nis talk about data structures.\\nAnd it's going to be\\nconcerned about not operating\\non constant amount of data at\\na time, like our CPU is doing,\\nbut instead, what it's\\ngoing to do is operate on--\\nstore a large amount of data\\nand support different operations\\non that data.\\nSo if I had a record\\nthat I want to maintain\\nto store those birthdays\\nthat we had before,\\nI might use something\\nlike a static array, which\\nyou guys maybe are not\\nfamiliar with, if you\\nhave been working in Python is\\nyour only programming language.\\nPython has a lot of really\\ninteresting data structures,\\nlike a list, and a\\nset, and a dictionary,\\nand all these kinds\\nof things that\\nare actually not in this model.\\nThere's actually a lot of code\\nbetween you and the computer,\\nand it's not always\\nclear how much time\\nthat interface is taking.\\nAnd so what we're going\\nto do starting on Thursday\\nis talk about ways of\\nstoring a non-constant amount\\nof information to\\nmake operations\\non that information faster.\\nSo just before you go,\\nI just want to give you\\na quick overview of the class.\\nTo solve an algorithms\\nclass-- an algorithm\\nproblem in this\\nclass, we essentially\\nhave two different strategies.\\nWe can either reduced to using\\nthe solution to a problem we\\nknow how to solve,\\nor we can design\\nour own algorithm,\\nwhich is going\\nto be recursive in nature.\\nWe're going to either put\\nstuff in the data structure\\nand solve a sorting problem,\\nor search in a graph.\\nAnd then, to design a\\nrecursive algorithm,\\nwe have various\\ndesign paradigms.\\nThis is all in your notes,\\nbut this is essentially\\nthe structure of the class.\\nWe're going to spend quiz 1,\\nthe first eight lectures on data\\nstructures and sorting.\\nSecond quiz will be on shortest\\npaths, algorithms, and graphs,\\nand then the last one will\\nbe on dynamic programming.\\nOK, that's the end\\nof the first lecture.\\nThanks for coming.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}